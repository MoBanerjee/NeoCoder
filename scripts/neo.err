01/03/2025 10:14:12 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.00s/it]
01/03/2025 10:14:25 - INFO - src.dp.dp_generator -   Using HF for inference
Inferencing DP:   0%|          | 0/199 [00:00<?, ?it/s]Inferencing DP:   7%|▋         | 14/199 [00:00<00:01, 132.96it/s]Inferencing DP:  15%|█▍        | 29/199 [00:00<00:01, 138.15it/s]Inferencing DP:  22%|██▏       | 43/199 [00:00<00:01, 137.08it/s]Inferencing DP:  29%|██▊       | 57/199 [00:00<00:01, 134.28it/s]Inferencing DP:  36%|███▌      | 72/199 [00:00<00:00, 138.56it/s]Inferencing DP:  43%|████▎     | 86/199 [00:00<00:00, 138.77it/s]Inferencing DP:  50%|█████     | 100/199 [00:00<00:00, 129.67it/s]Inferencing DP:  58%|█████▊    | 116/199 [00:00<00:00, 135.50it/s]Inferencing DP:  66%|██████▌   | 131/199 [00:00<00:00, 138.95it/s]Inferencing DP:  73%|███████▎  | 146/199 [00:01<00:00, 138.55it/s]Inferencing DP:  80%|████████  | 160/199 [00:01<00:00, 138.29it/s]Inferencing DP:  88%|████████▊ | 175/199 [00:01<00:00, 139.26it/s]Inferencing DP:  95%|█████████▌| 190/199 [00:01<00:00, 140.02it/s]Inferencing DP: 100%|██████████| 199/199 [00:01<00:00, 137.61it/s]
